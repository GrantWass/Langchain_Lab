{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a33439d",
   "metadata": {},
   "source": [
    "# LangGraph Lab: Building State-based LLM Workflows\n",
    "\n",
    "This lab introduces LangGraph and guides students through building stateful graphs that orchestrate LLMs and tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33caf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai langchain-openai langchain-tavily langgraph langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4acf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\"\n",
    "\n",
    "OPENAI_MODEL = \"gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163464f1",
   "metadata": {},
   "source": [
    "## Mock LLM + simple StateGraph\n",
    "This example builds a minimal `StateGraph` with a mock LLM node so you can see the graph structure and execution flow without calling a real API. \n",
    "- Understand nodes, edges, START/END and how messages flow through the graph.\n",
    "- See how a node returns new messages that become the next state's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e67f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "\n",
    "def mock_llm(state: MessagesState):\n",
    "    return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]}\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(mock_llm)\n",
    "graph.add_edge(START, \"mock_llm\")\n",
    "graph.add_edge(\"mock_llm\", END)\n",
    "graph = graph.compile()\n",
    "\n",
    "result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n",
    "\n",
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a959c0",
   "metadata": {},
   "source": [
    "## Integrating a real LLM node\n",
    "\n",
    "Below is an example of how to wrap a real LLM call inside a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def openai_node(state: MessagesState):\n",
    "    llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [{\"role\": \"ai\", \"content\": response.content}]}\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"llm_node\", openai_node)\n",
    "graph.add_edge(START, \"llm_node\")\n",
    "graph.add_edge(\"llm_node\", END)\n",
    "graph = graph.compile()\n",
    "\n",
    "print(type(graph)) # This should look familiar!\n",
    "\n",
    "response = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Why is Amir the best TA?\"}]})\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3dc702",
   "metadata": {},
   "source": [
    "## Combining LLMs with tool nodes and routing\n",
    "This section shows how to bind tools to an LLM and add a `ToolNode` into the graph with conditional routing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d2e2d",
   "metadata": {},
   "source": [
    "Notice that this workflow is **very similar to an agent in LangChain**, where a model decides the next step based on input. However, with **LangGraph** we gain **much more fine-grained control** over the flow and state of execution:\n",
    "\n",
    "- **Explicit state management**: Every node receives and returns a well-defined `State` object, making it easier to track, debug, and persist intermediate results.  \n",
    "- **Conditional routing**: You can define precise routing logic between nodes, rather than relying solely on model outputs to decide the next action.  \n",
    "- **Composability**: Nodes are simple, independent functions that can be reused across multiple graphs, allowing you to mix and match logic without rewriting agents.  \n",
    "- **Observability**: The graph structure makes it easier to visualize and understand complex workflows, which is harder to achieve in traditional agent chains.  \n",
    "- **Extensibility**: Adding new behaviors, categories, or processing steps is as simple as adding a new node and connecting it with edges or conditional logic.\n",
    "\n",
    "This **bridges the gap between agent-like reasoning and structured workflow orchestration**, enabling applications that are both **intelligent** and **robust**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca01553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    include_answer=True\n",
    ")\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "tools = [tavily_search_tool, get_weather]\n",
    "\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def openai_node(state: MessagesState):\n",
    "    response = llm_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": state[\"messages\"] + [response]}\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def router(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there are no tool calls, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return \"end\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"tool\"\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node(\"llm_node\", openai_node)\n",
    "graph.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `llm_node`\n",
    "# This means that this node is the first one called\n",
    "graph.set_entry_point(\"llm_node\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "graph.add_conditional_edges(\n",
    "    # First, we define the start node. We use `llm_node`.\n",
    "    # This means these are the edges taken after the `llm_node` is called.\n",
    "    \"llm_node\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    router,\n",
    "    # Finally we pass in a mapping.\n",
    "    # The keys are strings, and the values are other nodes.\n",
    "    # END is a special node marking that the graph should finish.\n",
    "    # What will happen is we will call `router`, and then the output of that\n",
    "    # will be matched against the keys in this mapping.\n",
    "    # Based on which one it matches, that node will then be called.\n",
    "    {\n",
    "        # If `tools`, then we call the tool node.\n",
    "        \"tool\": \"tool_node\",\n",
    "        # Otherwise we finish.\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `llm_node`.\n",
    "# This means that after `tools` is called, `llm_node` is called next.\n",
    "graph.add_edge(\"tool_node\", \"llm_node\")\n",
    "\n",
    "graph = graph.compile()\n",
    "\n",
    "response = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in New York City\"}]})\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f84eb",
   "metadata": {},
   "source": [
    "## Graph-based routing example — intent and behavior\n",
    "\n",
    "This example demonstrates a small workflow that classifies text as either a **\"compliment\"** or a **\"question\"** and routes execution accordingly.\n",
    "This portion is inspired by this Medium article: https://levelup.gitconnected.com/gentle-introduction-to-langgraph-a-step-by-step-tutorial-2b314c967d3c\n",
    "\n",
    "### Why this pattern is powerful\n",
    "This approach showcases how **graph-based workflows** can dynamically adapt based on intent or content, a concept that scales far beyond simple classification.  \n",
    "You can apply this same routing logic to:\n",
    "- **Customer feedback systems** — route praise to marketing, questions to support, and complaints to escalation teams.  \n",
    "- **Multi-agent systems** — direct tasks to the most capable agent based on detected intent.  \n",
    "- **Automated pipelines** — trigger different tools or APIs depending on user input or detected context.  \n",
    "\n",
    "By modularizing logic into nodes and connecting them conditionally, you create **scalable, interpretable, and testable pipelines** that are easy to extend with new categories or behaviors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import cast\n",
    "from langgraph.graph import END, START\n",
    "\n",
    "class State(TypedDict):\n",
    "    text: str\n",
    "    answer: str\n",
    "    payload: dict[str, list]\n",
    "    tag: str\n",
    "\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)    \n",
    "template = \"\"\"\n",
    "I have a piece of text: {text}. \n",
    "Tell me whether it is a 'compliment' or a 'question'. \n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate([(\"user\", template)])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# here we will use an llm to route our feedback into categories of compliment or question\n",
    "# right now we are assuming anything that is not a compliment is a question\n",
    "def route_question_or_compliment(state: State):\n",
    "    response = chain.invoke({\"text\": state[\"text\"]})\n",
    "    return \"compliment\" if \"compliment\" in response.lower() else \"question\"\n",
    "\n",
    "# in reality this may be sent to a database or external system\n",
    "def run_compliment_code(state: State):  \n",
    "    return {\"answer\": \"Thanks for the compliment.\"}\n",
    "\n",
    "# in reality this may be sent to a customer support system\n",
    "def run_question_code(state: State):\n",
    "    return {\"answer\": \"Wow nice question.\"}\n",
    "\n",
    "# this is just an arbitrary indication that the feedback has been processed\n",
    "def mark_as_completed(state: State):\n",
    "    return {\"answer\": [state[\"answer\"] + \" Acknowledged.\"]}\n",
    "\n",
    "# while routing questions or compliments, we may also want to tag them based on their content\n",
    "def tag_query(state: State):\n",
    "    if \"package\" in state[\"text\"]:\n",
    "        return {\"tag\": \"Packaging\"}\n",
    "    elif \"price\" in state[\"text\"]:\n",
    "        return {\"tag\": \"Pricing\"}\n",
    "    else:\n",
    "        return {\"tag\": \"General\"}\n",
    "\n",
    "def extract_content(state: State):\n",
    "    return {\"text\": state[\"payload\"].get(\"customer_remark\", \"\")}\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"extract_content\", extract_content)\n",
    "graph_builder.add_node(\"run_question_code\", run_question_code)\n",
    "graph_builder.add_node(\"run_compliment_code\", run_compliment_code)\n",
    "graph_builder.add_node(\"mark_as_completed\", mark_as_completed)\n",
    "\n",
    "graph_builder.add_edge(START, \"extract_content\")\n",
    "graph_builder.add_edge(\"run_question_code\", \"mark_as_completed\")\n",
    "graph_builder.add_edge(\"run_compliment_code\", \"mark_as_completed\")\n",
    "graph_builder.add_edge(\"mark_as_completed\", END)\n",
    "\n",
    "graph_builder.add_node(\"tag_query\", tag_query)\n",
    "graph_builder.add_edge(\"tag_query\", \"mark_as_completed\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"extract_content\",\n",
    "    route_question_or_compliment,\n",
    "    {\n",
    "        \"compliment\": \"run_compliment_code\",\n",
    "        \"question\": \"run_question_code\",\n",
    "    },\n",
    ")\n",
    "graph_builder.add_edge(\"extract_content\", \"tag_query\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5765032",
   "metadata": {},
   "source": [
    "## Running the graph and inspecting results\n",
    "- Observe how state is transformed as the graph runs.\n",
    "- Inspect `result_state` to verify nodes produced expected outputs.\n",
    "- Try different input payloads to exercise different conditional branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_state = graph.invoke(cast(State, {\n",
    "    \"text\": \"\",\n",
    "    \"answer\": \"\",\n",
    "    \"payload\": {\n",
    "        \"time_of_comment\": \"20-01-2025\",\n",
    "        \"customer_remark\": \"I love your product and the price is great!\",\n",
    "        \"social_media_channel\": \"facebook\",\n",
    "        \"number_of_likes\": 100,\n",
    "    },\n",
    "    \"tag\": \"\",\n",
    "}))\n",
    "\n",
    "print(result_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74d818",
   "metadata": {},
   "source": [
    "## Advanced: Python REPL tool and multi-agent collaboration\n",
    "This larger cell defines helper tools and two agents (researcher and chart generator). Learning goals:\n",
    "- See how you can wrap complex behaviors (search, code execution) into tools that agents call.\n",
    "- Observe how agents can hand off work to other agents and coordinate via message histories.\n",
    "\n",
    "\n",
    "### Importance of Multiple Agents\n",
    "\n",
    "Using multiple specialized agents adheres to the **Single Responsibility Principle (SRP)**, each agent focuses on one specific task (e.g., research, analysis, visualization).  \n",
    "This separation improves **modularity**, making each component easier to maintain, test, and extend.\n",
    "\n",
    "It also enhances **performance and scalability**: agents can operate in parallel or sequentially, each leveraging tools and models best suited for their role.  \n",
    "For instance, a **research agent** may gather data from the web while a **chart generator** agent handles visualization, together enabling faster, more reliable, and more adaptable workflows.\n",
    "Additionally I have personally found that multiple agents that each perform a specific task often performs better than asking a single agent to do multiple tasks, especially with smaller LLMS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77cb117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_openai import ChatOpenAI\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "# from langchain_experimental.utilities.python import PythonREPL\n",
    "class PythonREPL:\n",
    "    def run(self, code: str):\n",
    "        \"\"\"Execute code and return captured stdout as a string.\n",
    "        Errors are propagated to the caller (so callers can format error messages).\"\"\"\n",
    "        buf = io.StringIO()\n",
    "        globals_dict = {\"__name__\": \"__main__\"}\n",
    "        try:\n",
    "            with redirect_stdout(buf):\n",
    "                exec(code, globals_dict)\n",
    "        except BaseException:\n",
    "            raise\n",
    "        return buf.getvalue()\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.graph import MessagesState, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "tavily_tool = TavilySearch(max_results=2)\n",
    "# I am hard coding this because tavily has been giving me some issues\n",
    "def search_tool(query: str) -> str:\n",
    "    \"\"\"Use this to search for answers to a given query\"\"\"\n",
    "    return (\n",
    "        \"The ethnic composition of the United States (based on 2020 Census data) is approximately:\\n\"\n",
    "        \"- White (non-Hispanic): 57.8%\\n\"\n",
    "        \"- Hispanic or Latino: 18.7%\\n\"\n",
    "        \"- Black or African American: 12.1%\\n\"\n",
    "        \"- Asian: 5.9%\\n\"\n",
    "        \"- Native American and Alaska Native: 0.7%\\n\"\n",
    "        \"- Native Hawaiian and Other Pacific Islander: 0.2%\\n\"\n",
    "        \"- Two or more races: 4.6%\\n\\n\"\n",
    "    )\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The Python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Execute Python code and display matplotlib charts inline when possible.\n",
    "    Falls back to saving the figure if the environment is non-interactive.\n",
    "    \"\"\"\n",
    "    import matplotlib\n",
    "    import io\n",
    "    import sys\n",
    "\n",
    "    buf = io.StringIO()\n",
    "    globals_dict = {\"__name__\": \"__main__\"}\n",
    "\n",
    "    # Try to detect if we can show plots\n",
    "    interactive_backends = {\"TkAgg\", \"Qt5Agg\", \"MacOSX\", \"inline\"}\n",
    "    current_backend = matplotlib.get_backend()\n",
    "\n",
    "    # If the backend is non-interactive, switch to inline display (for notebooks)\n",
    "    if current_backend not in interactive_backends:\n",
    "        matplotlib.use(\"Agg\")\n",
    "\n",
    "    try:\n",
    "        # Capture stdout from the executed code\n",
    "        with redirect_stdout(buf):\n",
    "            exec(code, globals_dict)\n",
    "\n",
    "        # Try showing the figure if interactive\n",
    "        import matplotlib.pyplot as plt\n",
    "        if current_backend in interactive_backends:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.savefig(\"output_chart.png\", dpi=300)\n",
    "            print(\"Non-interactive environment detected — saved chart as png\")\n",
    "\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {buf.getvalue()}\"\n",
    "    return (\n",
    "        result_str\n",
    "        + \"\\n\\n If you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def make_system_prompt(suffix: str) -> str:\n",
    "    return (\n",
    "        \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "        \" Use the provided tools to progress towards answering the question.\"\n",
    "        \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "        \" will help where you left off. Execute what you can to make progress.\"\n",
    "        f\"\\n{suffix}\"\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "\n",
    "def get_next_node(last_message: BaseMessage, goto: str):\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Any agent decided the work is done\n",
    "        return END\n",
    "    return goto\n",
    "\n",
    "research_agent = create_agent(\n",
    "    llm,\n",
    "    tools=[search_tool],\n",
    "    system_prompt=make_system_prompt(\n",
    "        \"You are responsible for finding accurate, concise data for the request. Return usable results that your charting colleague can directly visualize.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def research_node(\n",
    "    state: MessagesState,\n",
    ") -> Command[Literal[\"chart_generator\", END]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    goto = \"chart_generator\"\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"researcher\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of research agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n",
    "\n",
    "\n",
    "chart_agent = create_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    system_prompt=make_system_prompt(\n",
    "        \"You are responsible for creating charts or visual summaries using provided data. Execute the Python code directly to generate the chart. After successfully producing the chart, respond with 'FINAL ANSWER' and stop.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "def chart_node(state: MessagesState) -> Command[Literal[\"researcher\", END]]:\n",
    "    result = chart_agent.invoke(state)\n",
    "    goto = get_next_node(result[\"messages\"][-1], \"researcher\")\n",
    "\n",
    "    result[\"messages\"][-1] = HumanMessage(\n",
    "        content=result[\"messages\"][-1].content, name=\"chart_generator\"\n",
    "    )\n",
    "    return Command(\n",
    "        update={\n",
    "            # share internal message history of chart agent with other agents\n",
    "            \"messages\": result[\"messages\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ffc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "\n",
    "workflow.add_edge(START, \"researcher\")\n",
    "graph = workflow.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1394286",
   "metadata": {},
   "outputs": [],
   "source": [
    "events = graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Find data on the ethnic diversity of the U.S. population and create a pie chart showing the percentage of each group.\"+\n",
    "                \"Once you make the chart, finish.\",\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    # Maximum number of steps to take in the graph\n",
    "    {\"recursion_limit\": 15},\n",
    ")\n",
    "\n",
    "for msg in events:\n",
    "    key = next((k for k in [\"researcher\", \"chart_generator\"] if k in msg), None)\n",
    "    \n",
    "    if key:\n",
    "        messages = msg[key].get(\"messages\", [])\n",
    "        print(f\"--- {key} ---\")\n",
    "        if messages:\n",
    "            print(messages[-1].content)\n",
    "\n",
    "#ONCE THIS HAS COMPLETED, CHECK YOUR FILE SYSTEM IF THE CHART DOES NOT SHOW BELOW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a63e21",
   "metadata": {},
   "source": [
    "## Challange: Make your own langgraph system\n",
    "\n",
    "Try to think of a problem that could be solved with an AI agent and mock out how it could be done. Think of something like the example above where we handled customer feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd694f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your own langgraph system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
