{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e937a4",
   "metadata": {},
   "source": [
    "# LangChain Lab: Exploring the Power of AI\n",
    "\n",
    "Objectives:\n",
    "- Install and configure a working environment for LangChain.\n",
    "- Use an LLM (via LangChain) with prompt templates and chains.\n",
    "- Explore agents and tools.\n",
    "- Complete exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a7818",
   "metadata": {},
   "source": [
    "## 0) Quick notes before you start\n",
    "\n",
    "- You will need an OpenAI API key (or another provider) to run the LLM examples.\n",
    "- Set `OPENAI_API_KEY` in your environment before running cells that call the LLM.\n",
    "- This notebook uses `langchain` APIs; install the package with the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d36579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.11/site-packages (1.0.3)\n",
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: tiktoken in /opt/homebrew/lib/python3.11/site-packages (0.12.0)\n",
      "Requirement already satisfied: faiss-cpu in /opt/homebrew/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: chromadb in /opt/homebrew/lib/python3.11/site-packages (1.3.3)\n",
      "Requirement already satisfied: langchain-openai in /opt/homebrew/lib/python3.11/site-packages (1.0.2)\n",
      "Requirement already satisfied: langchain-tavily in /opt/homebrew/lib/python3.11/site-packages (0.2.13)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (1.0.3)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (1.0.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/homebrew/lib/python3.11/site-packages (from langchain) (2.12.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.41)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /opt/homebrew/lib/python3.11/site-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/homebrew/lib/python3.11/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /opt/homebrew/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /opt/homebrew/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /opt/homebrew/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /opt/homebrew/lib/python3.11/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /opt/homebrew/lib/python3.11/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /opt/homebrew/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /opt/homebrew/lib/python3.11/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.8.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.8)\n",
      "Requirement already satisfied: idna in /opt/homebrew/lib/python3.11/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/homebrew/lib/python3.11/site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/lib/python3.11/site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/homebrew/lib/python3.11/site-packages (from faiss-cpu) (1.25.2)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.38.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /opt/homebrew/lib/python3.11/site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in /opt/homebrew/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/homebrew/lib/python3.11/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.2.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /opt/homebrew/lib/python3.11/site-packages (from langchain-tavily) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.11.14->langchain-tavily) (1.22.0)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/homebrew/lib/python3.11/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/lib/python3.11/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (2.42.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/homebrew/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/homebrew/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (6.33.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/homebrew/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.38.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.38.0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.59b0 in /opt/homebrew/lib/python3.11/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.59b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/lib/python3.11/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /opt/homebrew/lib/python3.11/site-packages (from tokenizers>=0.13.2->chromadb) (1.1.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.5.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (0.20.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/lib/python3.11/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/homebrew/lib/python3.11/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/homebrew/lib/python3.11/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/homebrew/lib/python3.11/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install langchain openai tiktoken faiss-cpu chromadb langchain-openai langchain-tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2424159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11 (main, Dec  3 2024, 17:20:40) [Clang 16.0.0 (clang-1600.0.26.4)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print('Python', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4af03",
   "metadata": {},
   "source": [
    "## 1) Simple LLM usage with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512cdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\"\n",
    "\n",
    "OPENAI_MODEL = \"gpt-5-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b095da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# from langchain_core.globals import set_debug\n",
    "# set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4539c362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'> <class 'langchain_openai.llms.base.OpenAI'> <class 'str'>\n",
      ".\n",
      "\n",
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\"], template=prompt_template\n",
    ")\n",
    "llm = OpenAI()\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\"funny\")\n",
    "\n",
    "print(type(chain), type(llm), type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e00dd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_openai.chat_models.base.ChatOpenAI'> <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Short answer\n",
      "Agentic AI is any artificial system that acts in the world (real or simulated) to pursue goals over time, choosing actions based on perception, models or learning, and taking feedback into account. It is ‚Äúagent‚Äëlike‚Äù ‚Äî autonomous, goal‚Äëdirected, and capable of planning and adapting.\n",
      "\n",
      "Why the term matters (intuitively)\n",
      "- A tool answers once and stops (e.g., a calculator returns a result).  \n",
      "- An agent continuously senses, decides, and acts to change its situation toward an objective (e.g., a vacuum robot that moves around, avoids obstacles, and tries to clean a room until it succeeds or its battery is low).  \n",
      "\n",
      "Key components of an agentic AI\n",
      "- Environment: what the agent perceives and acts upon.  \n",
      "- State (or observations): information the agent receives.  \n",
      "- Actions: choices the agent can make.  \n",
      "- Policy / Decision rule: how the agent picks actions from states.  \n",
      "- Objective / Reward: what the agent is trying to maximize (explicit or implicit).  \n",
      "- Model or learning mechanism: how the agent predicts consequences and improves decisions over time.  \n",
      "- Memory / planning: capability to use past experience or simulate future steps.\n",
      "\n",
      "A simple mathematical framework: Markov Decision Process (MDP)\n",
      "Many agentic systems are modeled as an MDP:\n",
      "- States S, actions A.  \n",
      "- Transition probability P(s' | s, a): probability of moving to state s' from s when action a is taken.  \n",
      "- Reward function R(s, a, s'): immediate reward (or utility) received for that transition.  \n",
      "- Policy œÄ(a | s): a rule that picks actions in states.  \n",
      "\n",
      "The agent seeks a policy œÄ that maximizes expected cumulative reward, often discounted:\n",
      "VœÄ(s) = E[ sum_{t=0}^‚àû Œ≥^t R(s_t,a_t,s_{t+1}) | s_0 = s, policy œÄ ],\n",
      "and the optimal value function V*(s) satisfies the Bellman optimality relation\n",
      "V*(s) = max_a E[ R(s,a,s') + Œ≥ V*(s') ].\n",
      "This captures planning: the agent considers immediate payoff plus future expected value.\n",
      "\n",
      "Variants and refinements\n",
      "- Model-free vs model-based: model-free learns value or policy directly from experience; model-based learns a model P and R and plans with it.  \n",
      "- Partial observability (POMDP): observations are noisy or incomplete, so the agent must infer hidden state.  \n",
      "- Continuous time/state/action, multi-agent settings, hierarchical agents (agents that contain subagents), and meta‚Äëlearning/reflexive agents are further extensions.\n",
      "\n",
      "Examples\n",
      "- Non-agentic: a chatbot that answers a single question without persistent goals or capability to take actions in the world beyond returning text.  \n",
      "- Agentic: a delivery robot that navigates streets, adapts route when blocked, and chooses tasks to maximize deliveries before battery runs out.  \n",
      "- Somewhere between: a spam filter that updates itself daily from new emails ‚Äî it acts (filters) and learns, but has limited autonomous decision scope.\n",
      "\n",
      "Why people care (benefits and risks)\n",
      "- Benefits: autonomy enables automation of complex, ongoing tasks (robotics, autonomous vehicles, adaptive controllers, automated research assistants).  \n",
      "- Risks: goal mis-specification, unintended side effects, long-term planning causing harmful shortcuts, distributional shift failures, multi-step deception if the agent models and manipulates humans. This motivates research in alignment, interpretability, safe exploration, and corrigibility.\n",
      "\n",
      "Short illustrative example (grid world)\n",
      "- States: positions on a grid.  \n",
      "- Actions: up/down/left/right.  \n",
      "- Reward: +1 at goal cell, 0 otherwise.  \n",
      "An agent uses value iteration to compute V*(s) and then follows the policy that moves toward high-value cells. That computation embodies planning and goal pursuit.\n",
      "\n",
      "Bottom line\n",
      "Agentic AI = systems that perceive, choose actions, and pursue objectives over time with some autonomy. The formal language of decision theory (MDPs, policies, value functions) neatly captures what agents do and provides tools to build and analyze them. If you want, I can walk through a concrete grid‚Äëworld example with equations or show how a simple agent learns by reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a math tutor. Explain the concept of {topic} clearly.\"\n",
    ")\n",
    "\n",
    "formatted_prompt = prompt.format_messages(topic=\"Agentic AI\")\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(type(llm), type(response))\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c353847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to the model\n",
      "human: \n",
      "Provide information about Belgium.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"capital\": {\"description\": \"capital of the country\", \"title\": \"Capital\", \"type\": \"string\"}, \"name\": {\"description\": \"name of the country\", \"title\": \"Name\", \"type\": \"string\"}}, \"required\": [\"capital\", \"name\"]}\n",
      "```\n",
      "\n",
      "-----------------------------------\n",
      "Output of the model: \n",
      "\n",
      "{\"name\":\"Belgium\",\"capital\":\"Brussels\"}\n",
      "The capital of Belgium is Brussels.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Country(BaseModel):\n",
    "    capital: str = Field(description=\"capital of the country\")\n",
    "    name: str = Field(description=\"name of the country\")\n",
    "\n",
    "PROMPT_COUNTRY_INFO = \"\"\"\n",
    "Provide information about {country}.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Country)\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "message = HumanMessagePromptTemplate.from_template(\n",
    "    template=PROMPT_COUNTRY_INFO,\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "# Format the prompt with the country and parser instructions\n",
    "chat_prompt_with_values = chat_prompt.format_prompt(\n",
    "    country=\"Belgium\",\n",
    "    format_instructions=parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "print(\"Input to the model\")\n",
    "for msg in chat_prompt_with_values.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "\n",
    "output = llm.invoke(chat_prompt_with_values.to_messages());\n",
    "\n",
    "country = parser.parse(output.content)\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output of the model: \\n\")\n",
    "print(output.content)\n",
    "print(f\"The capital of {country.name} is {country.capital}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31eb749c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='Inception' year=2010 director='Christopher Nolan' rating=8.8\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model.with_structured_output(Movie)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e0b83",
   "metadata": {},
   "source": [
    "## 2) Agents: LLMs + Tools\n",
    "\n",
    "Agents let a language model orchestrate tools (e.g., python REPL, search, calculator). Below is a minimal demonstration using LangChain's agent framework.\n",
    "\n",
    "https://docs.langchain.com/oss/python/langchain/agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b04d3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is the weather in sf\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  get_weather (call_Ujz5r5H6IoOEW7tt8hCbPvwB)\n",
      " Call ID: call_Ujz5r5H6IoOEW7tt8hCbPvwB\n",
      "  Args:\n",
      "    city: San Francisco, CA\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: get_weather\n",
      "\n",
      "It's always sunny in San Francisco, CA!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "For San Francisco (SF): \"It's always sunny in San Francisco, CA!\" \n",
      "\n",
      "Would you like current temperature, hourly forecast, or weather for a different location?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "\n",
    "print(type(agent))\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20f1d5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you mean a specific place called \"Raikes\"? A few possibilities:\n",
      "\n",
      "- The Raikes School (University of Nebraska‚ÄìLincoln)\n",
      "- Raikes Hall / Raikes Building at a particular university (e.g., houses certain departments)\n",
      "- Raikes Foundation (not a school ‚Äî usually not described as having ‚Äúprofessors‚Äù)\n",
      "- Something else (a high school or program named Raikes)\n",
      "\n",
      "Which one do you mean? If you tell me the institution or share a link, I can list the current professors (or give their profiles/contacts) for that Raikes program.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "response = llm.invoke(\"Who are the professors in Raikes\")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a20df0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who are the professors in Raikes\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Which \"Raikes\" do you mean? A few possibilities:\n",
      "\n",
      "- Raikes School (School of Computer Science & Management) at University of Nebraska‚ÄìLincoln  \n",
      "- Raikes Foundation (charity / leadership programs)  \n",
      "- Raikes Hall or another Raikes building at a specific university\n",
      "\n",
      "Tell me which one (and whether you want current faculty only, titles/contact info, or research areas), and I‚Äôll pull the list.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "import os\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    include_answer=True\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[tavily_search_tool],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "user_input = \"Who are the professors in Raikes\"\n",
    "response = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e54c046e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is 13781 times 2394\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "13781 times 2394 equals 32,995,214.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "#TODO: create an addition tool to give the model the ability to answer this question\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "user_input = \"What is 13781 times 2394\" # answer should be 32991714, model should get this wrong without a tool\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}\n",
    ")\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "        msg.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bfba292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "ask the model something here\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Do you mean you want me to formulate a question to send to another AI/model? If so, please tell me:\n",
      "\n",
      "1. Topic or domain (coding, math, writing, product design, debugging, general knowledge, etc.).\n",
      "2. Question type (open-ended, multiple choice, step-by-step problem, short answer).\n",
      "3. Difficulty or detail level (beginner, intermediate, expert).\n",
      "4. Any constraints or context to include (code language, word limit, facts/assumptions to provide).\n",
      "5. Whether you want an ideal answer or rubric along with the question.\n",
      "\n",
      "If you‚Äôre not sure, here are example questions I can use‚Äîpick one or tell me which style you want:\n",
      "\n",
      "- Coding (debugging): \"Here is a 30-line Python function that reverses words but produces incorrect output for some inputs. Identify the bug, explain why it fails, and provide a corrected implementation.\"\n",
      "- Math (intermediate): \"Prove that if a and b are integers such that a^2 + b^2 is divisible by 5, then both a and b are divisible by 5.\"\n",
      "- Creative writing (prompt): \"Write a 300-word scene in which a time traveler meets their younger self and chooses not to change the past; show the internal conflict.\"\n",
      "- Product brainstorming: \"Propose three feature ideas for a mobile app that helps urban cyclists navigate safer routes, with brief user stories and one metric to measure success per feature.\"\n",
      "- Short factual: \"Summarize the main differences between IPv4 and IPv6 in 4 bullet points.\"\n",
      "\n",
      "Which do you want, or give me your specifics and I‚Äôll create the question (and an ideal answer if desired).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "def your_tool(foo: str, bar: int) -> str:\n",
    "    \"\"\"\n",
    "        Write the function docs here. They are important!\n",
    "        This is how the model knows what the function does\n",
    "        Make sure to include the types in the function definition.\n",
    "        That is how the model knows what to pass in as arguements\n",
    "    \"\"\"\n",
    "    # TODO: Implement tool\n",
    "    return None\n",
    "\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[your_tool],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "user_input = \"ask the model something here\"\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}\n",
    ")\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46005d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langgraph.graph.state.CompiledStateGraph'>\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Extract contact info from: John Doe, john@example.com, (555) 123-4567\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  ContactInfo (call_QBAncp7FetcPij306ATB6ePf)\n",
      " Call ID: call_QBAncp7FetcPij306ATB6ePf\n",
      "  Args:\n",
      "    name: John Doe\n",
      "    email: john@example.com\n",
      "    phone: (555) 123-4567\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: ContactInfo\n",
      "\n",
      "Returning structured response: name='John Doe' email='john@example.com' phone='(555) 123-4567'\n",
      "\n",
      "\n",
      "Structured response from agent:\n",
      "name='John Doe' email='john@example.com' phone='(555) 123-4567'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[],\n",
    "    response_format=ToolStrategy(ContactInfo) #Change ToolStrategy with ProviderStrategy to see how output differs\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
    "})\n",
    "\n",
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n",
    "print(\"\\n\\nStructured response from agent:\")\n",
    "print(result[\"structured_response\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# === Mock tools ===\n",
    "def send_email_tool(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Pretend to send an email.\"\"\"\n",
    "    return f\"‚úÖ Email sent to {recipient} with subject '{subject}'.\"\n",
    "\n",
    "def read_email_tool() -> str:\n",
    "    \"\"\"Pretend to read an email.\"\"\"\n",
    "    return \"üìß You have 2 unread emails from Alice and Bob.\"\n",
    "\n",
    "tools = [send_email_tool, read_email_tool]\n",
    "\n",
    "# === Create the agent ===\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=tools,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_email_tool\": {\n",
    "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
    "                },\n",
    "                \"read_email_tool\": False,  # auto-approve reads\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "import random\n",
    "thread_id = random.randint(1, 1000000)\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Send an email to alice@example.com saying I‚Äôll be late to the meeting.\"}\n",
    "    ]\n",
    "}, {\"configurable\": {\"thread_id\": thread_id}})\n",
    "\n",
    "last_msg = result[\"messages\"][-1]\n",
    "if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "    tool_call = last_msg.tool_calls[0]\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    tool_args = tool_call[\"args\"]\n",
    "\n",
    "    print(f\"\\nProposed tool call: {tool_name}\")\n",
    "    for k, v in tool_args.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    decision = input(\"\\nApprove or reject? \").strip().lower()\n",
    "\n",
    "    if decision == \"approve\":\n",
    "        tool_func = next(t for t in tools if t.__name__ == tool_name)\n",
    "        output = tool_func(**tool_args)\n",
    "        followup = agent.invoke({\n",
    "            \"messages\": [\n",
    "                *result[\"messages\"],\n",
    "                {\"role\": \"tool\", \"name\": tool_name, \"content\": output, \"tool_call_id\": tool_call[\"id\"]},\n",
    "            ]\n",
    "        }, {\"configurable\": {\"thread_id\": thread_id}})\n",
    "        for msg in followup[\"messages\"]:\n",
    "            msg.pretty_print()\n",
    "    else:\n",
    "        print(\"‚ùå Tool call rejected.\")\n",
    "\n",
    "else:\n",
    "    for msg in result[\"messages\"]:\n",
    "        msg.pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
