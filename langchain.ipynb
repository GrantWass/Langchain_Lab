{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e937a4",
   "metadata": {},
   "source": [
    "# LangChain Lab: Exploring the Power of AI\n",
    "\n",
    "Objectives:\n",
    "- Install and configure a working environment for LangChain.\n",
    "- Use an LLM (via LangChain) with prompt templates and chains.\n",
    "- Explore agents and tools.\n",
    "- Complete exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a7818",
   "metadata": {},
   "source": [
    "## 0) Quick notes before you start\n",
    "\n",
    "- You will need an OpenAI API key (or another provider) to run the LLM examples.\n",
    "- Set `OPENAI_API_KEY` in your environment before running cells that call the LLM.\n",
    "- This notebook uses `langchain` APIs; install the package with the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c0232f",
   "metadata": {},
   "source": [
    "## Why these quick notes matter\n",
    "This block gives important pre-run information and prerequisites. It reminds students to provide an OpenAI API key and install dependencies before running cells that call the LLM; follow these steps to avoid runtime errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d36579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install langchain openai langchain-openai langchain-tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424159e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print('Python', sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4af03",
   "metadata": {},
   "source": [
    "## 1) Simple LLM usage with LangChain\n",
    "\n",
    "introduces basic LangChain interactions: creating prompts, calling an LLM, and reading responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512cdec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY_HERE\"\n",
    "\n",
    "OPENAI_MODEL = \"gpt-5-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea1ec8",
   "metadata": {},
   "source": [
    "This commented block demonstrates how to enable detailed logging/debugging for LangChain. Uncomment and run if you need more diagnostics while developing or investigating issues or are curious as to how some of the internals work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# from langchain_core.globals import set_debug\n",
    "# set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539c362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "prompt_template = \"Tell me a {adjective} joke\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"adjective\"], template=prompt_template\n",
    ")\n",
    "llm = OpenAI()\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(\"funny\")\n",
    "\n",
    "print(type(chain), type(llm), type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353877e8",
   "metadata": {},
   "source": [
    "## Prompt templates, chains, and simple output parsing\n",
    "This example demonstrates building a PromptTemplate, connecting it to an OpenAI LLM, and using a simple string output parser. It teaches how to compose small pipelines (prompt ‚Üí LLM ‚Üí parser) and call them using `invoke`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e00dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a math tutor. Explain the concept of {topic} clearly.\"\n",
    ")\n",
    "\n",
    "formatted_prompt = prompt.format_messages(topic=\"Agentic AI\")\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(type(llm), type(response))\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a0fd3",
   "metadata": {},
   "source": [
    "## Enforcing output formats ‚Äî a couple of approaches\n",
    "This block explains three common ways to get machine-readable, validated output from LLMs:\n",
    "\n",
    "1) System-prompt / instruction-based enforcement ‚Äî Add explicit instructions in the system or user prompt asking for a format (e.g., \"Return JSON with fields x,y,z\"). This is often works, but enforcement is weak (models may deviate). Use this for quick human-readable constraints.\n",
    "\n",
    "2) PydanticOutputParser (recommended for validation) ‚Äî Attach a `Pydantic` model and use `PydanticOutputParser.get_format_instructions()` to append precise format instructions to the prompt. After the model returns text, call `parser.parse(...)` to validate and convert into a typed Python object. This gives explicit format instructions and runtime validation (you get errors when output is invalid).\n",
    "\n",
    "3) `model.with_structured_output(PydanticModel)` ‚Äî Ask the model wrapper to always return the given Pydantic schema (the SDK handles converting/validating for you). This is convenient when you want the model API to return structured data directly.\n",
    "\n",
    "Quick notes on trade-offs:\n",
    "- Instruction-only (system prompt): easiest to write, least safe/validated.\n",
    "- PydanticOutputParser: explicit instructions + post-hoc validation; good balance of control and visibility.\n",
    "- with_structured_output: most convenient when supported by the SDK; hides some parsing details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c353847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Country(BaseModel):\n",
    "    capital: str = Field(description=\"capital of the country\")\n",
    "    name: str = Field(description=\"name of the country\")\n",
    "\n",
    "PROMPT_COUNTRY_INFO = \"\"\"\n",
    "Provide information about {country}.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Country)\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "message = HumanMessagePromptTemplate.from_template(\n",
    "    template=PROMPT_COUNTRY_INFO,\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([message])\n",
    "\n",
    "# Format the prompt with the country and parser instructions\n",
    "chat_prompt_with_values = chat_prompt.format_prompt(\n",
    "    country=\"Belgium\",\n",
    "    format_instructions=parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "print(\"Input to the model\") # This is interesting as we can see how \n",
    "for msg in chat_prompt_with_values.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "\n",
    "output = llm.invoke(chat_prompt_with_values.to_messages())\n",
    "\n",
    "country = parser.parse(output.content)\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Output of the model: \\n\")\n",
    "print(output.content)\n",
    "print(f\"The capital of {country.name} is {country.capital}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"A movie with details.\"\"\"\n",
    "    title: str = Field(..., description=\"The title of the movie\")\n",
    "    year: int = Field(..., description=\"The year the movie was released\")\n",
    "    director: str = Field(..., description=\"The director of the movie\")\n",
    "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
    "\n",
    "model_with_structure = model.with_structured_output(Movie)\n",
    "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e0b83",
   "metadata": {},
   "source": [
    "## 2) Agents: LLMs + Tools\n",
    "\n",
    "Agents let a language model orchestrate tools (e.g., python REPL, search, calculator). Below is a minimal demonstration using LangChain's agent framework.\n",
    "\n",
    "https://docs.langchain.com/oss/python/langchain/agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74caaf6a",
   "metadata": {},
   "source": [
    "## Agents: combining LLMs with tools\n",
    "Here we introduces agents ‚Äî systems where an LLM can call external tools (functions) to extend capabilities like search, computation, or side effects. The following cells contain concrete agent examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get weather for a given city.\"\"\"\n",
    "    return f\"It's always sunny in {city}!\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[get_weather],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n",
    ")\n",
    "\n",
    "print(type(agent))\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5e898",
   "metadata": {},
   "source": [
    "## Direct chat invocation example\n",
    "This short cell shows calling `ChatOpenAI.invoke` directly with a natural question. Use it to compare direct LLM responses versus agent-enabled behavior (with tools).\n",
    "\n",
    "This example quickly shows the power of enabling LLMs with tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0.3)\n",
    "\n",
    "response = llm.invoke(\"Who are the professors in Raikes\")\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20df0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "import os\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "# this essentially just allows the llm to search the web, real easy to set up\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=2,\n",
    "    topic=\"general\",\n",
    "    include_answer=True\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[tavily_search_tool],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "user_input = \"Who are the professors in Raikes\"\n",
    "response = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83148511",
   "metadata": {},
   "source": [
    "## Why agents need tools for math/computation\n",
    "This cell is an exercise showing a model answering a math question without a calculator tool. It highlights that LLMs can make arithmetic mistakes and motivates adding deterministic computation tools for correctness, highlighting another key use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "#TODO: create an addition tool to give the model the ability to answer this question\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools=[],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "user_input = \"What is 13781 times 2394\" # answer should be 32991714, model should get this wrong without a tool\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}\n",
    ")\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "        msg.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057ad75a",
   "metadata": {},
   "source": [
    "## Challange: Write your own tool\n",
    "This template shows the shape a tool should take (typed arguments and a clear docstring). The model uses the function signature and docs to know what arguments to pass ‚Äî implement the body to provide useful functionality.\n",
    "\n",
    "Attempt to create a tool to remedy something you have noticed that LLMs struggle with. If you aren't able to implement it in the time we have, that's fine, it is the insight that matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfba292",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "def your_tool(foo: str, bar: int) -> str:\n",
    "    \"\"\"\n",
    "        Write the function docs here. They are important!\n",
    "        This is how the model knows what the function does\n",
    "        Make sure to include the types in the function definition.\n",
    "        That is how the model knows what to pass in as arguements\n",
    "    \"\"\"\n",
    "    # TODO: Implement tool\n",
    "    return None\n",
    "\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[your_tool],\n",
    "    system_prompt=\"You are a helpful assistant\",\n",
    ")\n",
    "\n",
    "user_input = \"ask the model something here\"\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}\n",
    ")\n",
    "\n",
    "for msg in response[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630e9244",
   "metadata": {},
   "source": [
    "## Structured tool outputs and response schemas\n",
    "\n",
    "This example demonstrates how to return structured data from an agent using `ToolStrategy` or `ProviderStrategy` with a Pydantic model.\n",
    "\n",
    "By defining explicit response schemas, agents can produce consistent, machine-readable outputs rather than free-form text. This approach is valuable for a wide range of applications, such as:\n",
    "\n",
    "- **Data extraction** ‚Äî pulling specific fields or entities from unstructured input.  \n",
    "- **API integration** ‚Äî ensuring responses match required request/response formats.  \n",
    "- **Form filling and automation** ‚Äî mapping model outputs directly into databases or UI forms.  \n",
    "- **Validation and error handling** ‚Äî enforcing data types and constraints before downstream use.  \n",
    "- **Complex workflows** ‚Äî passing structured intermediate results between tools or agents.\n",
    "\n",
    "Structured output schemas improve reliability, enable automatic validation, and make it easier to compose AI components into larger, predictable systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46005d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Extract contact info from: John Doe, john@example.com, (555) 123-4567\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  ContactInfo (call_Be7KUhSPuwOTAomLY3TgChNN)\n",
      " Call ID: call_Be7KUhSPuwOTAomLY3TgChNN\n",
      "  Args:\n",
      "    name: John Doe\n",
      "    email: john@example.com\n",
      "    phone: (555) 123-4567\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: ContactInfo\n",
      "\n",
      "Returning structured response: name='John Doe' email='john@example.com' phone='(555) 123-4567'\n",
      "\n",
      "\n",
      "Structured response from agent:\n",
      "name='John Doe' email='john@example.com' phone='(555) 123-4567'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=[],\n",
    "    response_format=ToolStrategy(ContactInfo) #Change ToolStrategy with ProviderStrategy to see how output differs\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
    "})\n",
    "\n",
    "for msg in result[\"messages\"]:\n",
    "    msg.pretty_print()\n",
    "\n",
    "print(\"\\n\\nStructured response from agent:\")\n",
    "print(result[\"structured_response\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bee720",
   "metadata": {},
   "source": [
    "## Human-in-the-loop approval and tool execution\n",
    "This longer example shows middleware that prompts a human to approve or edit a tool call before execution. \n",
    "\n",
    "This example implements a checkpointer which allows us to track and come back to specific sessions as indicated by the thread_id.\n",
    "\n",
    "This Human-in-the-loop architecture is key for ensuring safe workflows where humans can limit the negative side effects of AI.\n",
    "\n",
    "Areas where this is applicable include:\n",
    "- **Financial decision-making**, where transactions or investments must be reviewed before commitment.  \n",
    "- **Healthcare applications**, where AI suggestions should be validated by medical professionals.  \n",
    "- **Data modification or deletion tasks**, where human approval prevents accidental or malicious actions.  \n",
    "- **Autonomous systems or robotics**, where human oversight ensures safety in physical environments.  \n",
    "- **Customer service and compliance**, where humans can approve sensitive responses or verify legal constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# === Mock tools ===\n",
    "def send_email_tool(recipient: str, subject: str, body: str) -> str:\n",
    "    \"\"\"Pretend to send an email.\"\"\"\n",
    "    return f\"‚úÖ Email sent to {recipient} with subject '{subject}'.\"\n",
    "\n",
    "def read_email_tool() -> str:\n",
    "    \"\"\"Pretend to read an email.\"\"\"\n",
    "    return \"üìß You have 2 unread emails from Alice and Bob.\"\n",
    "\n",
    "tools = [send_email_tool, read_email_tool]\n",
    "\n",
    "# === Create the agent ===\n",
    "agent = create_agent(\n",
    "    model=OPENAI_MODEL,\n",
    "    tools=tools,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_email_tool\": {\n",
    "                    \"allowed_decisions\": [\"approve\", \"edit\", \"reject\"],\n",
    "                },\n",
    "                \"read_email_tool\": False,  # auto-approve reads\n",
    "            }\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "import random\n",
    "thread_id = random.randint(1, 1000000)\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Send an email to alice@example.com saying I‚Äôll be late to the meeting.\"}\n",
    "    ]\n",
    "}, {\"configurable\": {\"thread_id\": thread_id}})\n",
    "\n",
    "last_msg = result[\"messages\"][-1]\n",
    "if hasattr(last_msg, \"tool_calls\") and last_msg.tool_calls:\n",
    "    tool_call = last_msg.tool_calls[0]\n",
    "    tool_name = tool_call[\"name\"]\n",
    "    tool_args = tool_call[\"args\"]\n",
    "\n",
    "    print(f\"\\nProposed tool call: {tool_name}\")\n",
    "    for k, v in tool_args.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    decision = input(\"\\nApprove or reject? \").strip().lower()\n",
    "\n",
    "    if decision == \"approve\":\n",
    "        tool_func = next(t for t in tools if t.__name__ == tool_name)\n",
    "        output = tool_func(**tool_args)\n",
    "        followup = agent.invoke({\n",
    "            \"messages\": [\n",
    "                *result[\"messages\"],\n",
    "                {\"role\": \"tool\", \"name\": tool_name, \"content\": output, \"tool_call_id\": tool_call[\"id\"]},\n",
    "            ]\n",
    "        }, {\"configurable\": {\"thread_id\": thread_id}})\n",
    "        for msg in followup[\"messages\"]:\n",
    "            msg.pretty_print()\n",
    "    else:\n",
    "        print(\"‚ùå Tool call rejected.\")\n",
    "\n",
    "else:\n",
    "    for msg in result[\"messages\"]:\n",
    "        msg.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9bafe",
   "metadata": {},
   "source": [
    "## CHALLENGE: Create your own agent\n",
    "\n",
    "- Try giving it a system prompt that seriously affects how it responds to prompts\n",
    "- Give it a couple tools to see when and how it uses them\n",
    "- Try something you were always curious about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9bd0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create your own agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
